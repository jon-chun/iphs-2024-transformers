# https://medium.com/@metechsolutions/setup-nvidia-gpu-in-ubuntu-22-04-for-llm-e181e473a3f4#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3NGRiYmE4ZmFlZTY5YWNhZTFiYzFiZTE5MDQ1MzY3OGY0NzI4MDMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMTI2MjM2OTg1MjU5MDYxODY3MzgiLCJlbWFpbCI6ImpvbmNodW4yMDAwQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJuYmYiOjE3MTc3MDE1MzQsIm5hbWUiOiJKb24gQ2h1biIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQ2c4b2NKRWNmT0hWRTJtUkVvbmg1aDEyTUNVWUlLYmlxQ0d3T25GLXhZYTZobGVMNldvPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6IkpvbiIsImZhbWlseV9uYW1lIjoiQ2h1biIsImlhdCI6MTcxNzcwMTgzNCwiZXhwIjoxNzE3NzA1NDM0LCJqdGkiOiI1YzUwYTcyNjM4MjZkMDgyNWZiZjEyNjllMzkyMDdhYTkwODc4ODhhIn0.QMs328UJG47Ah-gcucHnWPIe8vP38SwM0TMW6zegTJwpJUPMym_3pWfkmSBzehJQto1mAHn1vFlUJ34L4tERnL8MgCQ_I7e8pcrLuSuMgxBRRogoHZmuGXwLNuuW30gQj7kRJjcWxAp1B9LOa-eKbCphzSaTgOQck46wLP2_kvofLEJjobDnbGQDg2gVXEZrHA7BRgOuVzD2WBbRj2fd5iHkB-Z97drOeLeb4tMge55KKNUTzev-1F-bg8CH57-MOzWikI5UQhtmRwtgvlZrG3XRKHU1p2uKDoIERGsNXyEaIo8G12Zdo9tD9gpw5TRyf8UxEOg5QbK97Mh2sVMhKg

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_name = "NousResearch/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_new_tokens=64, do_sample=True, temperature=0.7)

question = "Question: What is the difference between a car and a truck?"
answer = pipe(question)
print(question)
print(answer[0]['generated_text'])